import { NextRequest, NextResponse } from 'next/server'
import { supabaseAdmin } from '@/lib/supabase'

interface ChatMessage {
  role: 'user' | 'assistant'
  content: string
  timestamp: string
}

export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ chatbotId: string }> }
) {
  try {
    const { chatbotId } = await params
    const { message, conversationHistory = [] } = await request.json()

    console.log('ğŸ’¬ Local chat request:', { 
      chatbotId, 
      message: message?.substring(0, 50),
      historyLength: conversationHistory.length 
    })

    if (!message?.trim()) {
      return NextResponse.json(
        { error: 'Message is required' },
        { status: 400 }
      )
    }

    // 1. Get merchant information only
    const { data: merchant, error: merchantError } = await supabaseAdmin
      .from('Merchant')
      .select(`
        id,
        businessName,
        welcomeMessage,
        primaryColor,
        dataSources:MerchantDataSource(
          type,
          title,
          url,
          isActive
        )
      `)
      .eq('chatbotId', chatbotId)
      .single()

    if (merchantError || !merchant) {
      console.log('âŒ Merchant not found:', merchantError?.message)
      return NextResponse.json({ 
        error: 'Merchant not found',
        response: 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØ¬Ø±.'
      }, { status: 404 })
    }

    console.log('âœ… Merchant found:', merchant.businessName)

    // 2. Generate AI response
    let aiResponse = generateSmartFallback(message, merchant.businessName, conversationHistory)
    let aiDebug: any = { success: false, error: 'not attempted', stage: 'init' }

    try {
      const chuteAIApiKey = process.env.CHUTES_AI_API_KEY

      if (!chuteAIApiKey) {
        aiDebug = { success: false, error: 'API key not found', stage: 'env_check' }
        console.log('âš ï¸ AI API key not found, using smart fallback')
        return NextResponse.json({ 
          response: aiResponse,
          debug: { aiDebug, merchant: merchant.businessName, fallbackUsed: true }
        })
      }

      console.log('ğŸš€ Sending enhanced streaming request with 128K tokens...')
      aiDebug.stage = 'calling_api'

      // ENHANCED WITH STREAMING AND HIGH TOKEN LIMIT
      const response = await fetch('https://llm.chutes.ai/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${chuteAIApiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'deepseek-ai/DeepSeek-V3-0324',
          messages: [
            {
              role: 'user',
              content: `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù…ØªØ¬Ø± "${merchant.businessName}". Ø±Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ ÙˆÙ…ÙÙŠØ¯ Ø¹Ù„Ù‰: ${message}`
            }
          ],
          stream: true,
          max_tokens: 128000,
          temperature: 0.7
        })
      })
      
      console.log(`ğŸ“¦ Status: ${response.status}`)
      
      if (response.ok) {
        console.log('âœ… Starting streaming response processing...')
        
        // Handle streaming response
        if (response.body) {
          const reader = response.body.getReader()
          const decoder = new TextDecoder()
          let fullResponse = ''
          
          try {
            while (true) {
              const { done, value } = await reader.read()
              
              if (done) {
                console.log('âœ… Streaming completed')
                break
              }
              
              const chunk = decoder.decode(value, { stream: true })
              const lines = chunk.split('\n')
              
              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const data = line.slice(6).trim()
                  
                  if (data === '[DONE]') {
                    console.log('âœ… Stream finished with [DONE]')
                    break
                  }
                  
                  try {
                    const parsed = JSON.parse(data)
                    const content = parsed.choices?.[0]?.delta?.content
                    
                    if (content) {
                      fullResponse += content
                    }
                  } catch (parseError) {
                    // Skip invalid JSON chunks
                    console.log('âš ï¸ Skipping invalid JSON chunk')
                  }
                }
              }
            }
            
            if (fullResponse.trim()) {
              aiResponse = fullResponse.trim()
              aiDebug = { 
                success: true, 
                error: null, 
                stage: 'streaming_success',
                contentLength: aiResponse.length,
                streaming: true,
                maxTokens: 128000
              }
              console.log('âœ… Streaming AI success!', { 
                responseLength: aiResponse.length,
                streaming: true 
              })
            } else {
              aiDebug = { 
                success: false, 
                error: 'No content in streaming response', 
                stage: 'no_streaming_content'
              }
              console.log('âŒ No content in streaming response')
            }
          } catch (streamError) {
            aiDebug = { 
              success: false, 
              error: `Streaming error: ${streamError instanceof Error ? streamError.message : String(streamError)}`, 
              stage: 'streaming_error'
            }
            console.log('âŒ Streaming processing error:', streamError)
          }
        } else {
          aiDebug = { 
            success: false, 
            error: 'No response body for streaming', 
            stage: 'no_stream_body'
          }
          console.log('âŒ No response body available for streaming')
        }
      } else {
        const errorData = await response.json()
        aiDebug = { 
          success: false, 
          error: `HTTP ${response.status}: ${JSON.stringify(errorData)}`, 
          stage: 'http_error',
          status: response.status
        }
        console.log('âŒ Error response:', errorData)
      }

    } catch (aiError) {
      aiDebug = { 
        success: false, 
        error: aiError instanceof Error ? aiError.message : String(aiError), 
        stage: 'exception',
        errorType: aiError instanceof Error ? aiError.constructor.name : typeof aiError
      }
      console.log('âŒ AI request failed:', aiError)
    }

    // 4. Return response (no database saving)
    return NextResponse.json({ 
      response: aiResponse,
      merchant: {
        businessName: merchant.businessName,
        primaryColor: merchant.primaryColor
      },
      timestamp: new Date().toISOString(),
      status: aiDebug.success ? 'success_ai_streaming' : 'success_fallback',
      debug: {
        ai: aiDebug,
        historyLength: conversationHistory.length,
        messageLength: message.length,
        fallbackUsed: !aiDebug.success,
        streaming: aiDebug.success,
        maxTokens: 128000
      }
    })

  } catch (error) {
    console.error('ğŸ’¥ Chat error:', error)
    return NextResponse.json(
      { 
        error: 'Internal server error',
        response: 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
        details: error instanceof Error ? error.message : 'Unknown error'
      },
      { status: 500 }
    )
  }
}

// Smart fallback response generator
function generateSmartFallback(message: string, businessName: string, conversationHistory: ChatMessage[]): string {
  const lowerMessage = message.toLowerCase().trim()
  
  // Check for greetings
  const greetings = ['Ù…Ø±Ø­Ø¨Ø§', 'Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…', 'Ø£Ù‡Ù„Ø§', 'ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±', 'Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±', 'hi', 'hello']
  if (greetings.some(greeting => lowerMessage.includes(greeting))) {
    const timeBasedGreeting = new Date().getHours() < 12 ? 'ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±' : 'Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±'
    return `${timeBasedGreeting}! Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…ØªØ¬Ø± ${businessName}. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ ğŸ˜Š`
  }
  
  // Check for product inquiries
  const productQuestions = ['Ø¹Ù†Ø¯ÙƒÙ…', 'Ù…ØªÙˆÙØ±', 'Ù…ÙˆØ¬ÙˆØ¯', 'Ø£Ø³Ø¹Ø§Ø±', 'ÙƒÙ… Ø³Ø¹Ø±', 'Ø¨ÙƒØ§Ù…', 'Ù…Ù†ØªØ¬Ø§Øª']
  if (productQuestions.some(word => lowerMessage.includes(word))) {
    return `Ø¨Ø§Ù„Ø·Ø¨Ø¹! Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø±Ø§Ø¦Ø¹Ø© ÙÙŠ ${businessName}. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØµÙØ­ Ù…ØªØ¬Ø±Ù†Ø§ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©. Ù‡Ù„ ØªØ¨Ø­Ø« Ø¹Ù† Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ØŸ`
  }
  
  // Check for questions
  const questionWords = ['ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'Ù…Ø§Ø°Ø§', 'Ù‡Ù„', 'Ù„ÙŠÙ‡', 'Ø¥Ø²Ø§ÙŠ']
  if (questionWords.some(word => lowerMessage.includes(word)) || lowerMessage.includes('ØŸ')) {
    return `Ø³Ø¤Ø§Ù„ Ù…Ù…ØªØ§Ø²! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ${businessName}. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø©ØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ`
  }
  
  // Check for complaints or negative words
  const negativeWords = ['Ù…Ø´ÙƒÙ„Ø©', 'Ø®Ø±Ø§Ø¨', 'Ø³ÙŠØ¡', 'ÙˆØ­Ø´', 'Ø­Ù…Ø§Ø±', 'ØºØ¨ÙŠ']
  if (negativeWords.some(word => lowerMessage.includes(word))) {
    return `Ø£Ø¹ØªØ°Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø¥Ø²Ø¹Ø§Ø¬. Ù†Ø­Ù† ÙÙŠ ${businessName} Ù†Ø­Ø±Øµ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø£ÙØ¶Ù„ Ø®Ø¯Ù…Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ØªØ¬Ø± Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø­Ù„ Ø£ÙŠ Ù…Ø´ÙƒÙ„Ø©. Ù†Ù‚Ø¯Ø± ØµØ¨Ø±Ùƒ ÙˆØªÙÙ‡Ù…Ùƒ ğŸ™`
  }
  
  // Check for thanks
  const thankWords = ['Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'ØªØ³Ù„Ù…', 'Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©']
  if (thankWords.some(word => lowerMessage.includes(word))) {
    return `Ø§Ù„Ø¹ÙÙˆ! Ø³Ø¹Ø¯Ø§Ø¡ Ø¨Ø®Ø¯Ù…ØªÙƒ ÙÙŠ ${businessName}. Ù†Ø­Ù† Ù‡Ù†Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù„Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ ğŸ˜Š`
  }
  
  // Check for contact/location questions
  const contactWords = ['ÙÙŠÙ†', 'Ø¹Ù†ÙˆØ§Ù†', 'Ù…ÙˆÙ‚Ø¹', 'ØªÙ„ÙŠÙÙˆÙ†', 'Ø±Ù‚Ù…', 'ØªÙˆØ§ØµÙ„']
  if (contactWords.some(word => lowerMessage.includes(word))) {
    return `ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ ${businessName} ÙÙŠ ØµÙØ­ØªÙ†Ø§ Ø£Ùˆ Ù…ØªØ¬Ø±Ù†Ø§. Ù†Ø­Ù† Ù…ØªØ§Ø­ÙˆÙ† Ù„Ø®Ø¯Ù…ØªÙƒ! ğŸ“`
  }
  
  // Check conversation history for context
  if (conversationHistory.length > 1) {
    const lastMessage = conversationHistory[conversationHistory.length - 2]?.content || ''
    if (lastMessage.includes('Ù…Ø±Ø­Ø¨') || lastMessage.includes('Ø£Ù‡Ù„Ø§')) {
      return `Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ ${businessName}ØŸ`
    }
  }
  
  // Default intelligent response
  const responses = [
    `Ø´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹ ${businessName}! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØªÙ‡.`,
    `Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ ${businessName}! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ`,
    `Ø£Ù‡Ù„Ø§Ù‹! Ø³Ø¹Ø¯Ø§Ø¡ Ø¨ÙˆØ¬ÙˆØ¯Ùƒ ÙÙŠ ${businessName}. Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ Ø§Ù„ÙŠÙˆÙ…ØŸ`
  ]
  
  return responses[Math.floor(Math.random() * responses.length)]
} 