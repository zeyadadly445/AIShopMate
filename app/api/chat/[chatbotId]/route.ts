import { NextRequest, NextResponse } from 'next/server'
import { supabaseAdmin } from '@/lib/supabase'

export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ chatbotId: string }> }
) {
  console.log('ğŸš€ Chat API with Local Storage (No DB Conversations)')
  
  try {
    // 1. Get chatbotId
    const { chatbotId } = await params
    console.log('ğŸ“ ChatbotId:', chatbotId)
    
    // 2. Parse request body
    const body = await request.json()
    const { message, sessionId, conversationHistory = [], stream = true } = body
    console.log('ğŸ“¥ Request:', { 
      hasMessage: !!message, 
      hasSessionId: !!sessionId, 
      stream,
      messageLength: message?.length,
      historyLength: conversationHistory?.length
    })

    // ğŸ” DEBUG: Log conversation history details
    console.log('ğŸ” CONVERSATION HISTORY DEBUG:')
    console.log('History Length:', conversationHistory?.length || 0)
    if (conversationHistory && conversationHistory.length > 0) {
      conversationHistory.forEach((msg: any, index: number) => {
        console.log(`Message ${index + 1}: [${msg.role}] "${msg.content?.substring(0, 50)}..."`)
      })
    } else {
      console.log('âŒ NO CONVERSATION HISTORY RECEIVED!')
    }
    console.log('Current Message:', message)

    // 3. Validate required fields
    if (!chatbotId || !message) {
      console.error('âŒ Missing required fields')
      return NextResponse.json({
        error: 'Missing required fields: chatbotId, message'
      }, { status: 400 })
    }

    // 4. Get merchant data (for business context only)
    console.log('ğŸ” Getting merchant data...')
    const { data: merchant, error: merchantError } = await supabaseAdmin
      .from('Merchant')
      .select(`
        id,
        businessName,
        welcomeMessage,
        primaryColor,
        isActive
      `)
      .eq('chatbotId', chatbotId)
      .single()

    if (merchantError || !merchant) {
      console.error('âŒ Merchant not found:', merchantError?.message)
      return NextResponse.json({
        error: 'Merchant not found',
        details: merchantError?.message
      }, { status: 404 })
    }
    
    console.log('âœ… Merchant found:', merchant.businessName)

    // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„ØªØ§Ø¬Ø±
    if (!merchant.isActive) {
      return NextResponse.json(
        { 
          response: 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.',
          reason: 'merchant_inactive'
        },
        { status: 403 }
      )
    }

    // 5. ÙØ­Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    const { data: limitsCheck, error: limitsError } = await supabaseAdmin
      .rpc('check_message_limits', { merchant_id: merchant.id })

    if (limitsError) {
      console.error('Error checking limits:', limitsError)
      return NextResponse.json(
        { error: 'Failed to check subscription limits' },
        { status: 500 }
      )
    }

    const limits = limitsCheck && limitsCheck[0]
    if (!limits || !limits.can_send) {
      console.log('ğŸš« Message limit reached:', limits?.reason)
      return NextResponse.json(
        { 
          response: limits?.reason === 'ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ' 
            ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ØºØ¯Ø§Ù‹.' 
            : 'Ø¹Ø°Ø±Ø§Ù‹ØŒ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø³Ù…ÙˆØ­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ØµØ§Ø­Ø¨ Ø§Ù„Ù…ØªØ¬Ø±.',
          redirectTo: `/chat/${chatbotId}/limit-reached`,
          reason: limits?.reason === 'ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ' ? 'daily_limit_reached' : 'monthly_limit_reached',
          limits: {
            daily_remaining: limits?.daily_remaining || 0,
            monthly_remaining: limits?.monthly_remaining || 0
          }
        },
        { status: 403 }
      )
    }

    // 6. Prepare AI context with conversation history from frontend
    console.log('ğŸ¤– Preparing AI context with local conversation history...')
    const businessContext = `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù…ØªØ¬Ø± "${merchant.businessName}". ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù‡Ø°Ø¨Ø© ÙˆÙ…ÙÙŠØ¯Ø© ÙˆÙ‚Ø¯Ù… Ø±Ø¯ÙˆØ¯ Ù…ÙØµÙ„Ø© ÙˆÙ…Ø³Ø§Ø¹Ø¯Ø©. ØªØ°ÙƒØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ.`
    
    // Use conversation history sent from frontend (last 25 messages)
    const recentHistory = conversationHistory?.slice(-25) || []
    console.log(`ğŸ“œ Using ${recentHistory.length} messages from local history`)

    // 7. Generate AI response
    if (stream) {
      // Return streaming response 
      return await generateStreamingResponse(message, businessContext, recentHistory, merchant)
    } else {
      // Return regular response
      const aiResponse = await generateRegularResponse(message, businessContext, recentHistory)

      // Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
      const { data: consumeResult, error: consumeError } = await supabaseAdmin
        .rpc('consume_message', { merchant_id: merchant.id })

      if (consumeError) {
        console.error('âŒ Error consuming message:', consumeError)
      } else {
        const result = consumeResult && consumeResult[0]
        if (result && result.success) {
          console.log('âœ… Message consumed successfully:', {
            dailyRemaining: result.daily_remaining,
            monthlyRemaining: result.monthly_remaining
          })
        }
      }

      return NextResponse.json({ 
        response: aiResponse,
        usage: consumeResult && consumeResult[0] ? {
          daily_remaining: consumeResult[0].daily_remaining,
          monthly_remaining: consumeResult[0].monthly_remaining
        } : undefined
      })
    }

  } catch (error) {
    console.error('ğŸ’¥ API Error:', error)
    console.error('ğŸ’¥ Error stack:', error instanceof Error ? error.stack : 'No stack')
    
    return NextResponse.json({
      error: 'Internal server error',
      message: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 })
  }
}

// Streaming response with Local Storage (No DB saving)
async function generateStreamingResponse(
  message: string, 
  context: string, 
  conversationHistory: any[],
  merchant: any
): Promise<Response> {
  console.log('ğŸŒŠ Starting streaming response (Local Storage Mode)...')

  // Use Environment Variables from Vercel
  const apiKey = process.env.CHUTES_AI_API_KEY
  const apiUrl = process.env.CHUTES_AI_API_URL || 'https://llm.chutes.ai/v1/chat/completions'
  const model = process.env.CHUTES_AI_MODEL || 'deepseek-ai/DeepSeek-V3-0324'

  console.log('ğŸ”‘ Using ENV:', {
    hasApiKey: !!apiKey,
    apiUrl,
    model,
    historyLength: conversationHistory?.length
  })

  if (!apiKey) {
    console.error('âŒ CHUTES_AI_API_KEY not found in environment variables')
    throw new Error('AI API key not configured')
  }

  // Build proper messages array for AI API (CORRECT FORMAT)
  const messages = []
  
  // 1. Add system message with business context
  messages.push({
    role: "system",
    content: context
  })

  // 2. Add conversation history as separate messages
  if (conversationHistory && conversationHistory.length > 0) {
    console.log('ğŸ”„ Adding conversation history to AI messages...')
    conversationHistory.forEach((msg: any, index: number) => {
      const cleanRole = msg.role === 'user' ? 'user' : 'assistant'
      messages.push({
        role: cleanRole,
        content: msg.content
      })
      console.log(`ğŸ“ Added history message ${index + 1}: [${cleanRole}] "${msg.content?.substring(0, 50)}..."`)
    })
    console.log(`ğŸ“ Added ${conversationHistory.length} history messages to AI context`)
  } else {
    console.log('âš ï¸ No conversation history to add!')
  }

  // 3. Add current user message
  messages.push({
    role: "user",
    content: message
  })

  console.log(`ğŸ¯ Total messages sent to AI: ${messages.length} (1 system + ${conversationHistory.length} history + 1 current)`)
  
  // ğŸ” DEBUG: Show final messages array structure  
  console.log('ğŸ” FINAL MESSAGES SENT TO AI:')
  messages.forEach((msg, index) => {
    console.log(`${index + 1}. [${msg.role}] "${msg.content?.substring(0, 100)}..."`)
  })

  try {
    const response = await fetch(apiUrl, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${apiKey}`,
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        model: model,
        messages: messages, // Use proper messages array format
        stream: true,
        max_tokens: 128000,
        temperature: 0.7
      })
    })

    console.log(`ğŸ“¦ AI API Status: ${response.status}`)

    if (!response.ok) {
      const errorText = await response.text()
      console.error('âŒ AI API error:', response.status, errorText)
      throw new Error(`AI API error: ${response.status}`)
    }

    // Create streaming response (no database saving)
    const stream = new ReadableStream({
      async start(controller) {
        const reader = response.body?.getReader()
        const decoder = new TextDecoder()
        let accumulatedContent = ''
        let chunkCount = 0
        let totalBytes = 0
        const startTime = Date.now()
        let lastChunkTime = startTime
        let maxDelay = 0
        let delayCount = 0

        if (!reader) {
          controller.error(new Error('No response body'))
          return
        }

        console.log('ğŸ¬ STREAMING PERFORMANCE MONITOR STARTED')
        console.log(`â° Start time: ${new Date(startTime).toISOString()}`)

        try {
          while (true) {
            const chunkStartTime = Date.now()
            const { done, value } = await reader.read()
            const chunkEndTime = Date.now()
            const readDelay = chunkEndTime - chunkStartTime
            
            if (done) {
              const totalTime = Date.now() - startTime
              console.log('âœ… STREAMING PERFORMANCE SUMMARY:')
              console.log(`ğŸ“Š Total time: ${totalTime}ms`)
              console.log(`ğŸ“¦ Total chunks: ${chunkCount}`)
              console.log(`ğŸ“ Total bytes: ${totalBytes}`)
              console.log(`ğŸ“ˆ Average chunk size: ${Math.round(totalBytes / chunkCount || 0)} bytes`)
              console.log(`â±ï¸ Average time per chunk: ${Math.round(totalTime / chunkCount || 0)}ms`)
              console.log(`ğŸŒ Max delay between chunks: ${maxDelay}ms`)
              console.log(`âš ï¸ Delays over 1000ms: ${delayCount}`)
              console.log(`ğŸ”¤ Final response length: ${accumulatedContent.length} characters`)
              
              // Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
              if (accumulatedContent.trim()) {
                const { data: consumeResult, error: consumeError } = await supabaseAdmin
                  .rpc('consume_message', { merchant_id: merchant.id })

                if (consumeError) {
                  console.error('âŒ Error consuming message:', consumeError)
                } else {
                  const result = consumeResult && consumeResult[0]
                  if (result && result.success) {
                    console.log('âœ… Message consumed successfully:', {
                      dailyRemaining: result.daily_remaining,
                      monthlyRemaining: result.monthly_remaining,
                      consumptionDetails: result.message
                    })
                  }
                }
              }
              
              controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'))
              controller.close()
              break
            }

            // Track timing and delays
            const timeSinceLastChunk = chunkStartTime - lastChunkTime
            if (timeSinceLastChunk > maxDelay) {
              maxDelay = timeSinceLastChunk
            }
            if (timeSinceLastChunk > 1000) { // Delays over 1 second
              delayCount++
              console.log(`ğŸŒ SIGNIFICANT DELAY: ${timeSinceLastChunk}ms since last chunk`)
            }

            chunkCount++
            totalBytes += value.length
            lastChunkTime = chunkEndTime

            // Log every 10th chunk for monitoring
            if (chunkCount % 10 === 0) {
              console.log(`ğŸ“¦ Chunk ${chunkCount}: ${value.length} bytes, read delay: ${readDelay}ms, since last: ${timeSinceLastChunk}ms`)
            }

            const chunk = decoder.decode(value, { stream: true })
            const lines = chunk.split('\n')

            for (const line of lines) {
              if (line.startsWith('data: ')) {
                const data = line.slice(6).trim()
                
                if (data === '[DONE]') continue
                
                try {
                  const parsed = JSON.parse(data)
                  const content = parsed.choices?.[0]?.delta?.content || ''
                  
                  if (content) {
                    accumulatedContent += content
                    
                    // Send chunk to client (will be saved in localStorage by frontend)
                    controller.enqueue(new TextEncoder().encode(`data: ${JSON.stringify({
                      content: content,
                      delta: content
                    })}\n\n`))
                  }
                } catch (parseError) {
                  console.error('Parse error:', parseError)
                }
              }
            }
          }
        } catch (streamError) {
          const errorTime = Date.now() - startTime
          console.error(`âŒ STREAMING ERROR after ${errorTime}ms:`, streamError)
          console.error(`ğŸ“Š Processed ${chunkCount} chunks, ${totalBytes} bytes before error`)
          controller.error(streamError)
        }
      }
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization'
      }
    })

  } catch (error) {
    console.error('âŒ Streaming error:', error)
    throw error
  }
}

// Regular response with Local Storage (No DB saving)
async function generateRegularResponse(message: string, context: string, conversationHistory: any[]): Promise<string> {
  console.log('ğŸ“ Generating regular response (Local Storage Mode)...')

  // Use Environment Variables from Vercel
  const apiKey = process.env.CHUTES_AI_API_KEY
  const apiUrl = process.env.CHUTES_AI_API_URL || 'https://llm.chutes.ai/v1/chat/completions'
  const model = process.env.CHUTES_AI_MODEL || 'deepseek-ai/DeepSeek-V3-0324'

  if (!apiKey) {
    console.warn('âŒ CHUTES_AI_API_KEY not found')
    return 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹.'
  }

  // Build proper messages array for AI API (CORRECT FORMAT)
  const messages = []
  
  // 1. Add system message with business context
  messages.push({
    role: "system",
    content: context
  })

  // 2. Add conversation history as separate messages
  if (conversationHistory && conversationHistory.length > 0) {
    conversationHistory.forEach((msg: any) => {
      messages.push({
        role: msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      })
    })
    console.log(`ğŸ“ Added ${conversationHistory.length} history messages to AI context`)
  }

  // 3. Add current user message
  messages.push({
    role: "user",
    content: message
  })

  console.log(`ğŸ¯ Total messages sent to AI: ${messages.length} (1 system + ${conversationHistory.length} history + 1 current)`)

  try {
    const response = await fetch(apiUrl, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${apiKey}`,
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        model: model,
        messages: messages, // Use proper messages array format
        stream: false,
        max_tokens: 128000,
        temperature: 0.7
      })
    })

    console.log(`ğŸ“¦ AI API Status: ${response.status}`)

    if (!response.ok) {
      const errorText = await response.text()
      console.error('âŒ AI API error:', response.status, errorText)
      throw new Error(`AI API error: ${response.status}`)
    }

    const data = await response.json()
    console.log("âœ… AI Response received (Local Storage Mode)")
    
    if (data.choices?.[0]?.message?.content) {
      let aiResponse = data.choices[0].message.content.trim()
      // Clean up response
      aiResponse = aiResponse.replace(/^(Ù…Ø³Ø§Ø¹Ø¯|Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯|Ø£Ù†Ø§|Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ?|Ø£Ù‡Ù„Ø§Ù‹ØŒ?)\s*/i, '')
      return aiResponse || 'Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹Ù†Ø§. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ'
    } else {
      throw new Error('Invalid AI response format')
    }

  } catch (error) {
    console.error('âŒ AI generation error:', error)
    return `Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹.`
  }
} 